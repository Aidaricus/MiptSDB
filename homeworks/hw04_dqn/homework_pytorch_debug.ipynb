{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqZ2EwnTZdC8"
      },
      "source": [
        "# Deep Q-Network implementation.\n",
        "\n",
        "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
        "\n",
        "Original paper:\n",
        "https://arxiv.org/pdf/1312.5602.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv7XJfXaZdC9"
      },
      "source": [
        "**This notebook is given for debug.** The main task is in the other notebook (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
        "\n",
        "**We suggest the following pipeline:** First implement debug notebook then implement the main one.\n",
        "\n",
        "**About evaluation:** All points are given for the main notebook with one exception: if agent fails to beat the threshold in main notebook you can still get points for beating the threshold in debug notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ioIEVODJZdC9",
        "outputId": "d084180e-1585-4d27-a71f-cda4a7ed5567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/atari_wrappers.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/utils.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/replay_buffer.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/framebuffer.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ[\"DISPLAY\"] = \":1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u8OFQOtGojc8",
        "outputId": "870520ea-fd26-4155-9e61-5ab16d53740e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZqlI3kZdC9"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dsYq558wZdC-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6ypPZ8e6ZdC-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8EGNlSZdC-"
      },
      "source": [
        "### CartPole again\n",
        "\n",
        "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
        "\n",
        "CartPole is the simplest one. It should take several minutes to solve it.\n",
        "\n",
        "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v-5u-CcQZdC-"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "\n",
        "def make_env():\n",
        "    # some envs are wrapped with a time limit wrapper by default\n",
        "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AmFXRrkqZdC-",
        "outputId": "9f18bff2-53ed-4bc6-aa68-8ee7b7210a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = make_env()\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n",
        "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOyWgOmvZdC-"
      },
      "source": [
        "### Building a network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqpThLZXZdC-"
      },
      "source": [
        "We now need to build a neural network that can map observations to state q-values.\n",
        "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UVlpkvZOZdC-",
        "outputId": "6c547dd3-15e5-4684-ac09-202b6c90eeb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# those who have a GPU but feel unfair to use it can uncomment:\n",
        "# device = torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RFva1cpyZdC-"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "        # Define your network body here. Please make sure agent is fully contained here\n",
        "        assert len(state_shape) == 1\n",
        "        state_dim = state_shape[0]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, n_actions)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        \"\"\"\n",
        "        takes agent's observation (tensor), returns qvalues (tensor)\n",
        "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
        "        \"\"\"\n",
        "        # Use your network to compute qvalues for given state\n",
        "        qvalues = self.net(state_t)\n",
        "\n",
        "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "        assert (\n",
        "            len(qvalues.shape) == 2 and\n",
        "            qvalues.shape[0] == state_t.shape[0] and\n",
        "            qvalues.shape[1] == n_actions\n",
        "        )\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, states):\n",
        "        \"\"\"\n",
        "        like forward, but works on numpy arrays, not tensors\n",
        "        \"\"\"\n",
        "        model_device = next(self.parameters()).device\n",
        "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
        "        qvalues = self.forward(states)\n",
        "        return qvalues.data.cpu().numpy()\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "\n",
        "        should_explore = np.random.choice(\n",
        "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bv1s5JKzZdC-"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vazC0DPQZdC_"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e-Sg1cqPZdC_"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
        "    \"\"\"Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward.\"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0NzjUEZdC_"
      },
      "source": [
        "### Experience replay\n",
        "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyCO4TuZdC_"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wQEHwR1AZdC_"
      },
      "outputs": [],
      "source": [
        "from replay_buffer import ReplayBuffer\n",
        "\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0RnFX5sfZdC_"
      },
      "outputs": [],
      "source": [
        "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
        "    Whenever game ends due to termination or truncation, add record with done=terminated and reset the game.\n",
        "    It is guaranteed that env has terminated=False when passed to this function.\n",
        "\n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "\n",
        "    :returns: return sum of rewards over time and the state in which the env stays\n",
        "    \"\"\"\n",
        "    s = initial_state\n",
        "    sum_rewards = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    for _ in range(n_steps):\n",
        "        qvalues = agent.get_qvalues([s])\n",
        "        action = agent.sample_actions(qvalues)[0]\n",
        "        next_obs, r, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        exp_replay.add(s, action, r, next_obs, done)\n",
        "        sum_rewards += r\n",
        "        if done:\n",
        "            next_obs, _ = env.reset()\n",
        "        s = next_obs\n",
        "\n",
        "    return sum_rewards, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZXXmFEKGZdC_",
        "outputId": "1f41fe73-b180-4f7e-a177-bc5a3ae4850f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-4bece36d6615>:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  states = torch.tensor(states, device=model_device, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well done!\n"
          ]
        }
      ],
      "source": [
        "# testing your code.\n",
        "exp_replay = ReplayBuffer(2000)\n",
        "\n",
        "state, _ = env.reset()\n",
        "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \" \"but instead added %i\" % len(\n",
        "    exp_replay\n",
        ")\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.1, (\n",
        "    \"Please make sure you restart the game whenever it is 'done' and \"\n",
        "    \"record the is_done correctly into the buffer. Got %f is_done rate over \"\n",
        "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (np.mean(is_dones), len(exp_replay))\n",
        ")\n",
        "\n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
        "    assert act_batch.shape == (10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
        "    assert reward_batch.shape == (10,), \"rewards batch should have shape (10,) but is instead %s\" % str(\n",
        "        reward_batch.shape\n",
        "    )\n",
        "    assert is_done_batch.shape == (10,), \"is_done batch should have shape (10,) but is instead %s\" % str(\n",
        "        is_done_batch.shape\n",
        "    )\n",
        "    assert [int(i) in (0, 1) for i in is_dones], \"is_done should be strictly True or False\"\n",
        "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
        "\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVGsnHRZdC_"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8BLJCNiuZdC_",
        "outputId": "b2f0f4f8-c31c-4c37-e7a5-5ecc55e87042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
        "# This is how you can load weights from agent into target network\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_GGShX3ZdC_"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hbg-xANZdC_"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "\n",
        "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
        "\n",
        "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VxrEOC7mZdC_",
        "outputId": "1c5b29b1-d523-4c1c-918d-2f75e95f2f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-17-422e267059e9>, line 29)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-422e267059e9>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    next_state_values = <YOUR CODE>\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
        "                    agent, target_network,\n",
        "                    gamma=0.99,\n",
        "                    check_shapes=False,\n",
        "                    device=device):\n",
        "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
        "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
        "    # shape: [batch_size, *state_shape]\n",
        "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
        "    is_done = torch.tensor(\n",
        "        is_done.astype('float32'),\n",
        "        device=device,\n",
        "        dtype=torch.float32,\n",
        "    )  # shape: [batch_size]\n",
        "    is_not_done = 1 - is_done\n",
        "\n",
        "    # get q-values for all actions in current states\n",
        "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # compute q-values for all actions in next states\n",
        "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # select q-values for chosen actions\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
        "\n",
        "    # compute V*(next_states) using predicted next q-values\n",
        "    next_state_values = <YOUR CODE>\n",
        "\n",
        "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
        "        \"must predict one value per state\"\n",
        "\n",
        "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "    # you can multiply next state values by is_not_done to achieve this.\n",
        "    target_qvalues_for_actions = <YOUR CODE>\n",
        "\n",
        "    # mean squared error loss to minimize\n",
        "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
        "            \"make sure you predicted q-values for all actions in next state\"\n",
        "        assert next_state_values.data.dim() == 1, \\\n",
        "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
        "            \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZKcPPnZdC_"
      },
      "source": [
        "Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp8eREoDZdC_"
      },
      "outputs": [],
      "source": [
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "\n",
        "loss = compute_td_loss(\n",
        "    obs_batch,\n",
        "    act_batch,\n",
        "    reward_batch,\n",
        "    next_obs_batch,\n",
        "    is_done_batch,\n",
        "    agent,\n",
        "    target_network,\n",
        "    gamma=0.99,\n",
        "    check_shapes=True,\n",
        ")\n",
        "loss.backward()\n",
        "\n",
        "assert loss.requires_grad and tuple(loss.data.size()) == (), \"you must return scalar loss - mean over batch\"\n",
        "assert np.any(\n",
        "    next(agent.parameters()).grad.data.cpu().numpy() != 0\n",
        "), \"loss must be differentiable w.r.t. network weights\"\n",
        "assert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A1QtGVqZdC_"
      },
      "source": [
        "### Main loop\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lAUT94JZdC_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOk81bdZZdC_"
      },
      "outputs": [],
      "source": [
        "seed = <YOUR CODE: your favourite random seed>\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13K5t2CTZdDA"
      },
      "outputs": [],
      "source": [
        "state_dim = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "state, _ = env.reset(seed=seed)\n",
        "\n",
        "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD7PAlwQZdDA"
      },
      "outputs": [],
      "source": [
        "REPLAY_BUFFER_SIZE = 10**4\n",
        "\n",
        "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "for i in range(100):\n",
        "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
        "        print(\n",
        "            \"\"\"\n",
        "            Less than 100 Mb RAM available.\n",
        "            Make sure the buffer size in not too huge.\n",
        "            Also check, maybe other processes consume RAM heavily.\n",
        "            \"\"\"\n",
        "        )\n",
        "        break\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
        "        break\n",
        "print(len(exp_replay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl2VCEYQZdDA"
      },
      "outputs": [],
      "source": [
        "# # for something more complicated than CartPole\n",
        "\n",
        "# timesteps_per_epoch = 1\n",
        "# batch_size = 32\n",
        "# total_steps = 3 * 10**6\n",
        "# decay_steps = 1 * 10**6\n",
        "\n",
        "# opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "# init_epsilon = 1\n",
        "# final_epsilon = 0.1\n",
        "\n",
        "# loss_freq = 20\n",
        "# refresh_target_network_freq = 1000\n",
        "# eval_freq = 5000\n",
        "\n",
        "# max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-sD-QyUZdDA"
      },
      "outputs": [],
      "source": [
        "timesteps_per_epoch = 1\n",
        "batch_size = 32\n",
        "total_steps = 4 * 10**4\n",
        "decay_steps = 1 * 10**4\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "init_epsilon = 1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 1000\n",
        "\n",
        "max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piqDfKQAZdDA"
      },
      "outputs": [],
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "grad_norm_history = []\n",
        "initial_state_v_history = []\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks8NAV8AZdDA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def wait_for_keyboard_interrupt():\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU3GSGZqZdDA"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "with trange(step, total_steps + 1) as progress_bar:\n",
        "    for step in progress_bar:\n",
        "        if not utils.is_enough_ram():\n",
        "            print('less that 100 Mb RAM available, freezing')\n",
        "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
        "            wait_for_keyboard_interrupt()\n",
        "\n",
        "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
        "\n",
        "        # play\n",
        "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
        "\n",
        "        # train\n",
        "        <YOUR CODE: sample batch_size of data from experience replay>\n",
        "\n",
        "        loss = <YOUR CODE: compute TD loss>\n",
        "\n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if step % loss_freq == 0:\n",
        "            td_loss_history.append(loss.data.cpu().item())\n",
        "            grad_norm_history.append(grad_norm)\n",
        "\n",
        "        if step % refresh_target_network_freq == 0:\n",
        "            # Load agent weights into target_network\n",
        "            <YOUR CODE>\n",
        "\n",
        "        if step % eval_freq == 0:\n",
        "            mean_rw_history.append(evaluate(\n",
        "                make_env(), agent, n_games=3, greedy=True, t_max=1000, seed=step)\n",
        "            )\n",
        "            initial_state_q_values = agent.get_qvalues(\n",
        "                [make_env().reset(seed=step)[0]]\n",
        "            )\n",
        "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
        "\n",
        "            clear_output(True)\n",
        "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
        "                (len(exp_replay), agent.epsilon))\n",
        "\n",
        "            plt.figure(figsize=[16, 9])\n",
        "\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.title(\"Mean reward per episode\")\n",
        "            plt.plot(mean_rw_history)\n",
        "            plt.grid()\n",
        "\n",
        "            assert not np.isnan(td_loss_history[-1])\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.title(\"TD loss history (smoothened)\")\n",
        "            plt.plot(utils.smoothen(td_loss_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 3)\n",
        "            plt.title(\"Initial state V\")\n",
        "            plt.plot(initial_state_v_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 4)\n",
        "            plt.title(\"Grad norm history (smoothened)\")\n",
        "            plt.plot(utils.smoothen(grad_norm_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwWFT2SBZdDA"
      },
      "outputs": [],
      "source": [
        "final_score = evaluate(make_env(), agent, n_games=30, greedy=True, t_max=1000)\n",
        "print(\"final score:\", final_score)\n",
        "assert final_score > 300, \"not good enough for DQN\"\n",
        "print(\"Well done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-feeX9YZdDA"
      },
      "source": [
        "**Agent's predicted V-values vs their Monte-Carlo estimates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjVuSIrPZdDA"
      },
      "outputs": [],
      "source": [
        "eval_env = make_env()\n",
        "record = utils.play_and_log_episode(eval_env, agent)\n",
        "print(\"total reward for life:\", np.sum(record[\"rewards\"]))\n",
        "for key in record:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCacwLw6ZdDA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(record[\"v_mc\"], record[\"v_agent\"])\n",
        "ax.plot(sorted(record[\"v_mc\"]), sorted(record[\"v_mc\"]), \"black\", linestyle=\"--\", label=\"x=y\")\n",
        "\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_title(\"State Value Estimates\")\n",
        "ax.set_xlabel(\"Monte-Carlo\")\n",
        "ax.set_ylabel(\"Agent\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}